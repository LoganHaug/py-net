{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga8ZwCa9x96W"
      },
      "source": [
        "#Neural Network Implementation#\n",
        "\n",
        "by Shivansh Rustagi and Max Alfano-Smith, Santa Cruz Artificial Intelligence\n",
        "derived from [this blog post](https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb)\n",
        "\n",
        "Today, we're going to be creating and training a deep neural network from the information we learned. We'll prepare a dataset, and once we are done making the neural net, we'll train it and make some predictons using the data we set up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLALhqXXjkJg"
      },
      "source": [
        "First: let's review the steps needed to create a Neural Network:\n",
        "\n",
        "\n",
        "\n",
        "1.   Prepare Data\n",
        "2.   Initialize Neural Network Layers\n",
        "3.   Activation functions (used to normalize outputs of neurons for next layer)\n",
        "4.   Forward Propagation (used for predictions)\n",
        "5.   Loss function (ensures we are moving in the right direction)\n",
        "6.   Backward Propagation (learning and training)\n",
        "7.   Updating parameters (optimize gradients)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZbamUKPkwL1"
      },
      "source": [
        "# 1. Prepare Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CkaiNN3bk1ft"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "# generate our training and testing labels and data!\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_dataset.data, iris_dataset.target, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8sGK5awk5bb"
      },
      "source": [
        "# 2. Initialize Neural Network Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z8E-W4dlk9OW"
      },
      "outputs": [],
      "source": [
        "# this creates the architecture for our neural network. notice how the\n",
        "# dimension of inputs for layer `n` is equal to the dimensions of the outputs of\n",
        "# layer `n-1`. the final layer has only one output, which is our prediction\n",
        "network_config = [\n",
        "  { 'input_dim': 2, 'output_dim': 25, 'activation': 'relu' },\n",
        "  { 'input_dim': 25, 'output_dim': 50, 'activation': 'relu' },\n",
        "  { 'input_dim': 50, 'output_dim': 50, 'activation': 'relu' },\n",
        "  { 'input_dim': 50, 'output_dim': 25, 'activation': 'relu' },\n",
        "  { 'input_dim': 25, 'output_dim': 1, 'activation': 'sigmoid' }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fHj7ZTabmvSA"
      },
      "outputs": [],
      "source": [
        "def init_layers(config, seed=69):\n",
        "  np.random.seed(seed)\n",
        "  network_num_layers = len(config)\n",
        "  network_params = {}\n",
        "\n",
        "  # enumerate is a python function which takes a dictionary and returns a\n",
        "  # collated representation of the keys:\n",
        "  # eg: {'a': 23, 'b': 42} -> [(0, 'a'), (1, 'b')]\n",
        "  # this makes it easier to iterate over a dictionary, which otherwise does not\n",
        "  # have any order\n",
        "  for idx, layer in enumerate(config):\n",
        "    # number the layers, starting at 1\n",
        "    layer_id = idx + 1\n",
        "    layer_in = layer['input_dim']\n",
        "    layer_out = layer['output_dim']\n",
        "\n",
        "    # the np.random.randn function creates an array prepopulated with values\n",
        "    # sampled from a normal distribution (standard deviation 1, mean 0)\n",
        "    # the size of the array is determined by the arguments passed\n",
        "\n",
        "    # TODO: randomly initialize the weights and biases of the network.\n",
        "    #       keep in mind that we are looping through the layers above, so this\n",
        "    #       should be done dynamically, by using the keys W1/b1, W2/b2 etc for\n",
        "    #       each layer in the dictionary\n",
        "    network_params['W' + str(layer_id)] = np.random.randn(layer_out, layer_in) * 0.1\n",
        "    network_params['b' + str(layer_id)] = np.random.randn(layer_out, 1) * 0.1\n",
        "\n",
        "  return network_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc2YUBcxmjMG"
      },
      "source": [
        "# 3. Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhs8SdI1mpPd"
      },
      "outputs": [],
      "source": [
        "# TODO: write the sigmoid function\n",
        "# YOUR CODE BELOW\n",
        "from math import e\n",
        "\n",
        "def sigmoid(Z):\n",
        "  return 1 / (1 + e ** -Z)\n",
        "\n",
        "# TODO: write the relu function negative numbers are mapped to zero, otherwise\n",
        "# the value remains the same\n",
        "# YOUR CODE BELOW\n",
        "def relu(Z):\n",
        "  if Z <= 0:\n",
        "    return 0\n",
        "  return Z\n",
        "\n",
        "# sigmoid derviative, which we will use during the backpropagation step\n",
        "def sigmoid_derivative(dA, Z):\n",
        "  return dA * sigmoid(Z) * (1 - sigmoid(Z))\n",
        "\n",
        "# relu derviative, which we will use during the backpropagation step\n",
        "def relu_derivative(dA, Z):\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0\n",
        "  return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q_CGrVxueo9"
      },
      "source": [
        "# 4. Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEYVqXyswD1D"
      },
      "outputs": [],
      "source": [
        "# execute the forward propagation step for one layer\n",
        "# this is a helper function for the `full_forward_prop function`\n",
        "def single_layer_forward_prop(A_prev, W, b, activation='relu'):\n",
        "  # matrix multiply the weights with the old activation signal, and add bias\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # take this new intermediate result and calculate its activation signal\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  return A, Z\n",
        "\n",
        "# complete forward propagation step, used to run inferences\n",
        "def full_forward_prop(X, params, config):\n",
        "  # this is temporary \"memory\"/\"cache\" dictionary, which will store, among other\n",
        "  # things, the temporary activation signals and matrices we need to carry out\n",
        "  # backpropagation\n",
        "  temp = {}\n",
        "  # the activation signal for the first layer is simply the input\n",
        "  A = X\n",
        "\n",
        "  # iterate over the layers to carry out forward propagation\n",
        "  for idx, layer in enumerate(config):\n",
        "    layer_id = idx + 1\n",
        "    # get the activation signal from the previous layer\n",
        "    A_prev = A\n",
        "\n",
        "    # get activation function, weights, and bias for the current layer\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # carry out forward propagation for one layer\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # store the activation signal and intermediate matrix in memory\n",
        "    temp['A' + str(idx)] = A_prev\n",
        "    temp['Z' + str(layer_id)] = Z\n",
        "\n",
        "  return A, temp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61akOhHP03Zz"
      },
      "source": [
        "# 5. Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgN_ayF_0_2p"
      },
      "outputs": [],
      "source": [
        "# simply a python conversion of the Binary Cross Entropy Loss function\n",
        "# we are using this loss function because it is ideal for choosing between\n",
        "# two classes\n",
        "def cost_fn(y_hat, y):\n",
        "  m = y_hat.shape[1]\n",
        "  # TODO: Write the cost function! this can be found in the slides as well\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # the np.squeeze function gets rid of extra dimensions of an array\n",
        "  # eg: [[[[[1, 2, 3]], [[4, 5, 6]]]]] --> [[1, 2, 3], [4, 5, 6]]\n",
        "  return np.squeeze(cost)\n",
        "\n",
        "# a utility function which takes probabilities and changes them into classes\n",
        "def prob_to_class(p):\n",
        "  p_ = np.copy(p)\n",
        "  p_[p_ > 0.5] = 1\n",
        "  p_[p_ <= 0.5] = 0\n",
        "  return p_\n",
        "\n",
        "# calculate the accuracy\n",
        "def accuracy_fn(y_hat, y):\n",
        "    y_hat_ = prob_to_class(y_hat)\n",
        "    # (y_hat_ == y) returns a matrix of boolean values, comparing each entry\n",
        "    # in both matrices\n",
        "    # once this is done, .all returns the `logical and` across axis 0, and \n",
        "    # computes the mean to give us our value for accuracy\n",
        "    return (y_hat_ == y).all(axis = 0).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDv59c6S7-EL"
      },
      "source": [
        "# 6. Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMMfl-X48AJJ"
      },
      "outputs": [],
      "source": [
        "# backpropagation step for a single layer\n",
        "def single_layer_back_prop(dA, W, b, Z, A_prev, activation=\"relu\"):\n",
        "  # get number of data points\n",
        "  m = A_prev.shape[1]\n",
        "  \n",
        "  # math -> python\n",
        "  dZ = relu_derivative(dA, Z) if activation == 'relu' else sigmoid_derivative(dA, Z)\n",
        "  dW = np.dot(dZ, A_prev.T) / m\n",
        "  db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "  dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "  return dW, db, dA_prev\n",
        "\n",
        "def full_back_prop(y_hat, y, temp, params, config):\n",
        "  # dictionary we will use to calculate gradients\n",
        "  gradients = {}\n",
        "  m = y.shape[1]\n",
        "  # ensure prediction and truth value vectors have same shape\n",
        "  y = y.reshape(y_hat.shape)\n",
        "\n",
        "  # TODO: differential of loss with respect to prediction, also found in the\n",
        "  # slides\n",
        "  # YOUR CODE HERE:\n",
        "\n",
        "  # as implied in the algorithm name, we want to work backward and use what we \n",
        "  # have learned to compute the derivatives of the cost function with respect to\n",
        "  # the weights \n",
        "  for idx, layer in reversed(list(enumerate(config))):\n",
        "    layer_id = idx + 1\n",
        "    activation = layer['activation']\n",
        "\n",
        "    dA = dA_prev\n",
        "\n",
        "    # TODO recover the parameters for a given layer\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # TODO: carry out a single step of backpropagation on this layer\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # add the gradients we have calculated to be used later\n",
        "    gradients[\"dW\" + str(layer_id)] = dW\n",
        "    gradients[\"db\" + str(layer_id)] = db\n",
        "  \n",
        "  return gradients\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAT5dAgvCjas"
      },
      "source": [
        "# 7. Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9iW2mzsCmRN"
      },
      "outputs": [],
      "source": [
        "# the actual learning part: using the gradients we have calculated to update the\n",
        "# weights and biases\n",
        "def update_params(params, config, gradients, learning_rate):\n",
        "  # this time, we want to start the enumeration at 1 because we are moving up\n",
        "  # the layers\n",
        "  for idx, layer in enumerate(config, 1):\n",
        "    # the learning rate is a hyperparameter which controls how fast or slowly\n",
        "    # the parameters are updated (as you can see below, it directly scales by\n",
        "    # how much the gradient is subtracted)\n",
        "    # a super low learning rate could lead to the network never \"converging\"\n",
        "    # and reaching an optimal solution, but a learning rate which is too high\n",
        "    # could lead to DIVERGENCE, which is also very bad\n",
        "    \n",
        "    # TODO: UPDATE THE PARAMETERS USING THE CALCUATED GRADIENTS AND LEARNING RATE\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARF97JtDMUM"
      },
      "source": [
        "# Finally... the training step!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmFvkXvrDOfj"
      },
      "outputs": [],
      "source": [
        "def train(X, y, config, epochs, learning_rate):\n",
        "  # create the network\n",
        "  params = init_layers(config)\n",
        "  # collect the cost and accuracy throughout training so that we can plot it if \n",
        "  # we want\n",
        "  cost_history = []\n",
        "  accuracy_history = []\n",
        "\n",
        "  # an epoch is a hyperparameter which siginifies the number of complete passes\n",
        "  # over the training dataset\n",
        "  for i in range(epochs):\n",
        "    # put together everything we wrote up there!\n",
        "    y_hat, mem = full_forward_prop(X, params, config)\n",
        "    \n",
        "    # store accuracy and cost for later\n",
        "    cost_history.append(cost_fn(y_hat, y))\n",
        "    accuracy_history.append(accuracy_fn(y_hat, y))\n",
        "\n",
        "    # TODO: CALCULATE GRADIENTS USING FULL BACKPROP AND UPDATE THE PARAMETERS\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "  # we return the parameters here: aka, the newly trained weights and biases\n",
        "  # for each layer of the network. We can use this set of optimized parameters\n",
        "  # to run inference on new data, which is done below!\n",
        "  return params, cost_history, accuracy_history \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI6j7y4QFG-Q"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SYyGcRoJchv"
      },
      "outputs": [],
      "source": [
        "# TODO: train this bad boy! note: you will have to transpose the X and Y vectors\n",
        "#       to get them in the right shape. You can do this with `np.transpose` \n",
        "# YOUR CODE HERE:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWpqT0Cl7mmV"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# quickly just plot accuracy over training\n",
        "plt.plot(accuracy_history)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.title('accuracy over training period')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrAWJSKTTKhV"
      },
      "outputs": [],
      "source": [
        "# run inferences on test data!\n",
        "# remember notation: y_hat is the predicted values, and we predict these values\n",
        "# by sending our test data through the network via the FORWARD PROPAGATION step.\n",
        "y_test_hat, _ = full_forward_prop(np.transpose(X_test), params, network_config)\n",
        "\n",
        "# calculate and print final test accuracy\n",
        "test_accuracy = accuracy_fn(y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
        "print(\"Test set accuracy: {:.2f}\".format(test_accuracy))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Logan H Neural Nets Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
